[Basic]
random_seed = 0
cuda_id     = 0

base_path   = "../models/final-test"
folder_name = "transformer-mlm-bllip"

[Corpus]
tag_type = "mlm"

    [Corpus.UDGNBLLIPTextCorpus]
    base_path = "/path/to/bliip_87_89_wsj_LDC2000T43.tgz"
    in_memory = true
    mode = 'XS' # 'XS', 'SM', 'MD', 'LG'

[Embeddings]

    [Embeddings.AutoMLMOneHotEmbeddings]
    embedding_length = 256
    with_mask        = true

[SequenceTagger]
tagger           = "MaskedLanguageModel"
reuse_embedding_weight = true

    [SequenceTagger.module]
    name             = "TransformerEncoder"
    d_model          = 256
    d_ff             = 2048
    n_layers         = 4
    n_head           = 14
    d_qkv            = 128
    dropout          = 0.15
    pos_embed        = "add"

[Trainer]
trainer              = "MaskedLanguageModelTrainer"
learning_rate        = 0.0002
mini_batch_size      = 32
max_epochs           = 200
shuffle              = true
eps                  = 1e-9
weight_decay         = 3.5e-6

anneal_factor        = 0.5
patience             = 5
min_learning_rate    = 0.0000001

fix_dev_mask         = true
fix_test_mask        = true