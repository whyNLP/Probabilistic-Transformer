[Basic]
random_seed = 0
cuda_id     = 0

base_path   = "../models/final-test"
folder_name = "transformer-mlm-ptb"

[Corpus]
tag_type = "mlm"

    [Corpus.StandardPTBCorpus]

[Embeddings]

    [Embeddings.AutoMLMOneHotEmbeddings]
    embedding_length = 384
    with_mask        = true

[SequenceTagger]
tagger           = "MaskedLanguageModel"
reuse_embedding_weight = true

    [SequenceTagger.module]
    name             = "TransformerEncoder"
    d_model          = 384
    d_ff             = 2048
    n_layers         = 5
    n_head           = 8
    d_qkv            = 256
    dropout          = 0.15
    pos_embed        = "add"

[Trainer]
trainer              = "MaskedLanguageModelTrainer"
learning_rate        = 0.0001
mini_batch_size      = 16
max_epochs           = 200
shuffle              = true
eps                  = 1e-9
weight_decay         = 1.2e-6

anneal_factor        = 0.5
patience             = 5
min_learning_rate    = 0.0000001

fix_dev_mask         = true
fix_test_mask        = true